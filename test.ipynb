{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc28a6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gstmchen/miniconda3/envs/d1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the same model as in sft_train.py\n",
    "model_name = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\", trust_remote_code=True, use_fast=True)\n",
    "\n",
    "# Load the s1.1K dataset\n",
    "dataset = load_dataset(\"simplescaling/s1K-1.1\", split=\"train\")\n",
    "\n",
    "# SYSTEM_PROMPT as in sft_trainer.py\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "Your reasoning here\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "input_token_lengths = []\n",
    "prompt_token_lengths = []\n",
    "n = len(dataset)\n",
    "\n",
    "for i in range(n):\n",
    "    question = SYSTEM_PROMPT + \"\\n\\n\" + dataset[i][\"question\"]\n",
    "    trajectory = f\"<reasoning>{dataset[i]['deepseek_thinking_trajectory']}</reasoning>\\n<answer>{dataset[i]['deepseek_attempt']}</answer>\"\n",
    "    prompt = [{\"role\": \"user\", \"content\": question}]\n",
    "    response = [{\"role\": \"assistant\", \"content\": trajectory}]\n",
    "    # Full input (prompt + response)\n",
    "    full_input = tokenizer.apply_chat_template(prompt + response, tokenize=False)\n",
    "    # Prompt only\n",
    "    prompt_only = tokenizer.apply_chat_template(prompt, tokenize=False) + \"\\n\"\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer(full_input, return_tensors=\"pt\").input_ids\n",
    "    prompt_ids = tokenizer(prompt_only, return_tensors=\"pt\").input_ids\n",
    "    input_token_lengths.append(input_ids.shape[1])\n",
    "    prompt_token_lengths.append(prompt_ids.shape[1])\n",
    "\n",
    "# Plot the length distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(input_token_lengths, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Input Token Lengths (Prompt + Response)\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(prompt_token_lengths, bins=30, color='salmon', edgecolor='black')\n",
    "plt.title(\"Prompt Token Lengths\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da8db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gstmchen/miniconda3/envs/d1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the s1.1K dataset\n",
    "dataset = load_dataset(\"simplescaling/s1K-1.1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa01133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let me try to work through these problems one by one. They all seem related to sparse sets, oracles, and these P_angel classes. Let's start with the first one.\n",
      "\n",
      "**Problem 1:** Given k sparse sets S_1, S_2, ..., S_k, we need to construct a single sparse set S and a poly-time TM M with oracle access to S such that M can decide whether x is in S_i given <x,i> as input.\n",
      "\n",
      "Hmm. Each S_i is sparse, so for each length n, each S_i has at most p(n) strings of length n. Since there are k sparse sets, maybe we combine them into S in some way. The challenge is to encode all the S_i's into S so that, given x and i, we can query S to check if x is in S_i. Also, S itself must be sparse.\n",
      "\n",
      "How do we combine multiple sparse sets into one and keep the combined set sparse? Let's think. Suppose each S_i has at most p(n) strings per length n. If k is a constant, then the total number for each n would be k*p(n). If S is the union of all S_i's, but tagged with their index i. Wait, but the union might not be sparse if the tags increase the length? Wait, maybe we can encode the pair (i, x) into a string in S such that when given x and i, we can form a specific query to the oracle S.\n",
      "\n",
      "But we have to be careful not to blow up the length too much. For example, maybe for each x in S_i, encode it in S as i concatenated with x, but then the length would be log k + |x|. However, since k is a constant, the length of the encoded string would be O(|x|), so for each n, the number of strings in S of length m = log k + n would be the sum over i of |S_i^{=n}|, which is at most k*p(n). Since m is roughly n, and p(n) is a polynomial, then the total number for length m would be also poly(m), preserving sparsity.\n",
      "\n",
      "Wait, perhaps more carefully: Let the new set S be { (i, x) | x ∈ S_i }, where (i, x) is some encoding of the pair i and x into a binary string. For the encoding, maybe preprend the binary representation of i, which is log k bits, to x. Then, each such string is of length log k + |x|. Since S_i is sparse, for each length n, S_i has at most p(n) strings. For each n, the number of strings in S of length log k + n is the sum over i=1 to k of |S_i^{=n}|. Since each S_i has at most p(n) strings of length n, total is k*p(n). Since k is a constant, k*p(n) is still a polynomial in n, but m = log k + n is roughly n, so we can express k*p(n) as a polynomial in m, which is O(n). Since p(n) is a polynomial, k*p(n) is poly(m). Thus, S is sparse.\n",
      "\n",
      "Then, the oracle machine M, when given <x,i>, encodes i and x into the string s = (i, x), and queries S whether s is in S. If yes, then x was in S_i, otherwise not. But is the encoding invertible? As long as the encoding is such that from the string s, we can recover i and x, but actually, the machine just needs to compute s from i and x, not the other way.\n",
      "\n",
      "Wait, but the problem doesn't say that k is given as part of the input. Wait, the input is <x, i>, so i is part of the input, which can be written in binary, and assuming that the input i is between 1 and k.\n",
      "\n",
      "Wait, but how does S know about k? Because in the problem statement, given S_1,...,S_k (finite k), we need to build S. If the question is for fixed k, then it's manageable. If k is part of the input, that is, the problem is parameterized by k, then we might need to handle variable k. But the problem says \"Given k ∈ N sparse sets S_1,...,S_k\", so k is fixed for each instance. So S can be constructed based on k.\n",
      "\n",
      "So yes, in this case, if we construct S as the union of tagged strings (i,x) where i ranges from 1 to k and x is in S_i, then S is sparse. Then, when given <x, i>, the machine encodes (i,x) into a string s (using some delimiter or fixed-length encoding for i), then makes an oracle query for s in S, and accepts if the oracle says yes.\n",
      "\n",
      "Therefore, the answer is constructing S as the union of the tagged sets, and M simply encodes x and i into a tagged string and queries S.\n",
      "\n",
      "**Problem 2:** Define P_bad-angel as P_angel but with the angel string α_n computable in poly-time. Is P = P_bad-angel? Is NP = P_bad-angel?\n",
      "\n",
      "First, understand what P_angel is. P_angel allows a poly-length advice string α_n for each input length n, similar to P/poly, but here the advice is fixed for all inputs of length n, and the machine runs in poly-time. But in the bad-angel version, α_n must be computable in polynomial time by an algorithm A(n).\n",
      "\n",
      "So P_bad-angel is the class of languages decidable by a poly-time machine with a poly-length advice string that can be generated in poly-time. Wait, but if the advice is generated in polynomial time, then why not just incorporate that polynomial-time computation into the main machine? Because the advice can be computed on the fly. Wait, but the difference is that in the bad-angel case, the advice is computed as a function only of n, not the input. However, since A can compute α_n in poly-time given n, then in the machine M(x, α_n), the advice α_n can be computed in poly-time from n, which is polynomial in |x|.\n",
      "\n",
      "So if the advice can be generated in polynomial time given the input length (i.e., 1^n), then the advice isn't giving any extra power beyond P. Because the machine can compute the advice itself. Therefore, P_bad-angel would equal P.\n",
      "\n",
      "Wait, let's formalize that. Let L be in P_bad-angel. Then there exists a poly-time M, and a poly-time A, such that α_n = A(n). Then, for input x, x is in L iff M(x, α_{|x|}) = 1. But since α_{|x|} can be computed in poly-time given |x|, which is the length of x, then the combined machine would first compute α_{|x|} from x (by running A on n = |x|, which is poly(n) time), then run M(x, α_n). Since both A and M are poly-time, the total time is polynomial in |x|. Therefore, L is in P, so P_bad-angel ⊆ P. Conversely, P ⊆ P_bad-angel because we can ignore the angel string (set α_n to some fixed string, which can be computed trivially). Therefore, P = P_bad-angel.\n",
      "\n",
      "Regarding NP: If NP = P_bad-angel? Well, NP is different. In the angel model, you have this poly-length string that can depend on n, but here, since it's computable in poly-time, there's no nondeterminism. Since the angel string is computable in poly-time, then the advice doesn't give any additional power beyond P. But NP is a class that includes problems verifiable in poly-time with a certificate, which can vary with the input. In this case, since the angel string is fixed for each input length, and computed deterministically, it's like a P machine.\n",
      "\n",
      "So unless NP = P, which is unknown, NP would not equal P_bad-angel. But according to the above, P_bad-angel = P, so NP = P_bad-angel would mean NP = P, which is conjectured to be false. Thus, the answer is P = P_bad-angel, and NP ≠ P_bad-angel unless NP = P.\n",
      "\n",
      "But the problem says \"Is NP = P_bad-angel? Justify.\" So unless the user allows that α_n could encode SAT solutions or something, but since α_n is computed in poly time from n, you can't encode solutions for all possible formulas of size n because SAT is NP-complete. Thus, likely NP is not equal to P_bad-angel.\n",
      "\n",
      "**Problem 3:** Given L ∈ P_angel, show there's a sparse set S_L and a poly-time TM M with oracle access to S_L that decides L.\n",
      "\n",
      "Hmm. Since L is in P_angel, there exists a poly p, a sequence {α_n} where |α_n| = p(n), and a poly-time M such that x ∈ L iff M(x, α_n) = 1. So for each length n, α_n is the advice. Now we need to construct a sparse oracle S_L such that a poly-time machine with access to S_L can decide L.\n",
      "\n",
      "Sparse sets are sets with few strings per length. The idea might be to encode the advice strings α_n into S_L. But α_n is of length p(n) for each n. How to do that.\n",
      "\n",
      "Note that S_L needs to have at most poly(m) strings of length m. One approach is for each n, encode α_n into a small number of strings in S_L. Since the advice α_n is of polynomial length in n, to make S_L sparse, you might spread the bits of α_n across multiple strings in S_L. For example, you can have for each n, a set of strings that encode the advice α_n. But since α_n is length p(n), perhaps use p(n) strings, each encoding a bit of α_n. However, then S_L would have p(n) strings for each n, but this might not be sparse if p(n) is a polynomial. Because for each n, we have p(n) strings of some length related to n, but over all lengths, the total number for each length would be polynomial.\n",
      "\n",
      "Wait, we need S_L to be sparse, meaning that for each length m, |S_L^{=m}| ≤ q(m) for some polynomial q.\n",
      "\n",
      "So how to encode α_n's into a sparse set S_L.\n",
      "\n",
      "Idea: Let's represent α_n as part of the oracle in a way that for input x of length n, the machine can query the oracle for α_n. But α_n is of length p(n), which is polynomial in n, so to store α_n, the oracle could have a string of length p(n) + log n, which serves as a key (n) and the value α_n. Wait, but how to index n?\n",
      "\n",
      "Alternatively, for each n, have in S_L a string that encodes n and the advice α_n. But encoding n can be done via binary representation. Then for each n, the number of strings in S_L for that encoding is polynomial, but the set must be sparse.\n",
      "\n",
      "Let's think. For each n, let’s denote a unique string that encodes n followed by the entire advice string α_n. So for example, the string could be bin(n) concatenated with α_n, where bin(n) is the binary representation of n. Then the length of each such string is log n + p(n).\n",
      "\n",
      "Now, for m = log n + p(n), for each m, how many n's satisfy log n + p(n) = m? If p(n) is polynomial, e.g., p(n) = n^k, then for each m, can there be multiple n with log n + n^k ≈ m? Unlikely. For example, m is dominated by n^k, so for sufficiently large n, n is roughly m^{1/k}, so for each m, there's at most one n (unless the encoding is overlapping). Hence, the number of strings of length m is O(1), which is polynomial (constant).\n",
      "\n",
      "But wait, the length log n + p(n) would vary for each n. For example, for different n's, their bin(n) representations have different length. But maybe if we fixed the encoding to use the same number of bits for each n. Wait, but log n is variable length. If instead, we represent n with O(log n) bits, but pad it to a fixed length. But perhaps in the original problem, there is no need for encoding; rather, we can structure S_L as a set where for each n, there's a unique string that somehow encodes α_n, but in a way that the number of strings for each length is bounded by a polynomial.\n",
      "\n",
      "Alternatively, partition the advice string α_n into blocks of length O(log n), and store each block with an index. Since α_n is p(n) bits, dividing into (p(n)/log n) blocks. Each block is of length O(log n), and there are polynomially many blocks. Then, the sparse set S_L can have each block indexed by n and position, hence for each n, the number of strings is O(p(n)/log n), which if p(n) is polynomial, e.g., p(n) = n^k, then O(n^k / log n), but this is still not polynomial per length. Wait.\n",
      "\n",
      "Alternatively, let's encode the entire α_n as a single string for each n, but stored using a unique identifier, then the number of strings in S_L per length would be polynomially bounded.\n",
      "\n",
      "Wait, but if we have α_n for each n, even if α_n is polynomial length, there's one string per n. So the number of strings of length p(n) is one for each n. So, consider m = p(n). Since p is a polynomial, for each m, there can be multiple n such that p(n) = m. However, since p is a polynomial, then for each fixed m, the equation p(n) = m has at most O(1) solutions (depending on the polynomial). For example, p(n) = n^k. Then for a given m, n = m^{1/k}, assuming m is a perfect kth power. Otherwise, no solution. Hence, for each m, there could be at most one string. So for m being a polynomial value, the number of strings of length m is at most 1, which is sparse. Then, the set S_L = { α_n | n ∈ N }, with each α_n being a string of length p(n). Wait, but in this case, the number of strings of length p(n) is 1 per n. However, the polynomial p(n) could map different n to the same m. For example, p(n) = n^2, then n=2 and n=3 would give p(n) = 4 and 9, different. So if p is injective beyond a certain n, then each m=p(n) has at most one n, so |S_L^{=m}| ≤1, which is certainly sparse. Thus, S_L is sparse since each m is hit by at most once. So putting S_L = { α_n | n ∈ N } would make each m (of the form p(n)) contain one string, which is sparse. But how to retrieve α_n given n. However, to get α_n, the machine would need to query the oracle S_L for a string of length p(n), which is exactly the string α_n. But constructing that seems impossible if we need to get the bits of α_n from the oracle. Because a query to the oracle is asking whether a particular string is in S_L, but α_n is of length p(n), so to recover α_n, the machine would need to iterate over all possible strings of length p(n) and check each against the oracle. Which is exponential in p(n), which is not feasible.\n",
      "\n",
      "So, perhaps the encoding has to be different. Let's think of ways the oracle can recover α_n efficiently. Maybe structure the oracle S_L as an encoding where for each bit of α_n, we can query the oracle for something that relates to that bit. For example, for each n, create entries in S_L with the form (n, i, b) where b is the ith bit of α_n. Then, the machine M, given x of length n, needs to get the bits of α_n, which is p(n) long, by making queries of the form (n, i, 0) and (n, i, 1) for each i from 1 to p(n), but how to do that with a sparse set. Since for each n and i, only one of (n, i, 0) or (n, i, 1) would be in S_L, the bits of α_n.\n",
      "\n",
      "However, the problem is that the encoding would need to be keyed on (n, i), but converting (n, i) to a string. If n and i are encoded in binary, then the length of each such string is O(log n + log i). Since i ranges up to p(n), which is polynomial, log i is O(log n). Hence, each string is of length O(log n), but the total number of such strings for each n is p(n), which is polynomial. For a given length m = O(log n), how many strings of length m are in S_L? Since m is log n + c, we can invert to n ≈ 2^{m - c}. The number of strings for n would be p(n) ≈ p(2^m) = 2^{O(m)}, which is exponential in m. Therefore, S_L would not be sparse.\n",
      "\n",
      "Therefore, this approach doesn't work. Another idea is to use pairwise hash functions or some indexing scheme. Wait, but we need a way to encode α_n into a sparse oracle that allows a poly-time machine to access the bits. Another approach might be to store α_n in the oracle using a binary search tree structure, but this could complicate.\n",
      "\n",
      "Wait, maybe break α_n into smaller chunks, and store each chunk at a position that depends on n. For instance, α_n is a p(n)-bit string, split it into p(n)/k chunks, each of k bits, then store each chunk with an index (n, i). But again, this requires handling indices, and similar length as before.\n",
      "\n",
      "Alternatively, store the entire α_n in the oracle as a single string at a position determined by n. So, for each n, we have an entry in S_L of the form n followed by α_n. The issue is that n can be given in binary as log n bits, and α_n is p(n) bits, so the length is log n + p(n). But then, for each length m = log n + p(n), how many entries are there? For varying n, m will vary, and for each m, at most how many n's have log n + p(n) = m?\n",
      "\n",
      "But depends on the growth of p(n). For example, if p(n) is monotonic and grows polynomially, then for each m, n is roughly (m)^(1/k) given p(n) = n^k. Hence, m = log n + n^k ≈ n^k for large n. Hence, m will be unique for large n, so again, each m corresponds to at most one n. Hence, S_L is sparse since |S^{=m}| ≤1 for each m. Therefore, if S_L consists of the set { bin(n) concatenated with α_n | n ∈ N }, then this set is sparse.\n",
      "\n",
      "To retrieve α_n, the oracle machine, given input x of length n, would need to form the string bin(n) concatenated with a dummy string (since α_n is fixed and determined by n). Wait, no, because the machine M doesn't have prior knowledge of α_n. The machine M needs to use the oracle S_L to retrieve information about α_n. How can the machine M retrieve α_n if it's stored as bin(n) + α_n?\n",
      "\n",
      "Wait, M has to create the string bin(n) + something and check if it's in S_L. But since the rest of the string is α_n, which M doesn't know, this seems paradoxical. Unless M can iterate through possibilities, but since the length of α_n is p(n), constructing bin(n) + α_n would be a string of length log n + p(n). Since p(n) is polynomial, the total length is polynomial in n. So, the machine M cannot generate all possible candidates for the suffix, as it is exponential in p(n).\n",
      "\n",
      "Alternatively, since the original M in P_angel takes x and α_n as input, and runs in time poly(|x|). If instead, we have an oracle S_L that contains the string bin(n) concatenated with α_n, then the actual bits of α_n can be encoded in the oracle string. For example, the oracle string is s = bin(n) + α_n, so to retrieve α_n, M can compute bin(n) and then generate s by appending alpha_n's bits. But since alpha_n is not known, how can M form s? Again, unless there's a unique s that starts with bin(n), but then M can generate prefixes bin(n) and then query all possible extensions. But that's exponential.\n",
      "\n",
      "This seems not helpful. Let's reorient.\n",
      "\n",
      "Perhaps apply the result from Problem 1. In Problem 1, k sparse sets can be combined into a single sparse oracle. For Problem 3, each L in P_angel is decided by a machine M using advice α_n. Maybe reduce this to having an oracle that encodes all the possible advice strings.\n",
      "\n",
      "However, these advice strings α_n are of polynomial size. If we could store each α_n as a single entry in a sparse set S_L, then whenever the machine needs the advice, it can query S_L for that α_n.\n",
      "\n",
      "But storing α_n as a string would require S_L to have one string of length p(n) for each n. If p(n) is a polynomial, for m = p(n), that corresponds to one string of length m per n equal to p^{-1}(m). However, if p is invertible (polynomial and strictly increasing), then for each m, there may be at most one n s.t. p(n) = m. Hence, in that case, S_L would be sparse. For example, if p(n) = n^k, then for each m, n is m^{1/k}, which may not be integer, so for m which is a perfect kth power, there's n, otherwise not. So for S_L = { α_n | n ∈ N }, each string is of length p(n), hence the number of strings of length m is 0 or 1. Therefore, S_L is sparse.\n",
      "\n",
      "But then how does M access the correct α_n? When given x of length n, the machine needs to get α_{n}. Since α_{n} is the string of length p(n), to query S_L, the machine can try generating all possible strings of length p(n) and check which one is in S_L. But enumerating all strings of length p(n) is exponential (there are 2^{p(n)} strings), which is not feasible.\n",
      "\n",
      "This suggests the previous approach is not useful. Another thought: can we use the fact that M uses the advice α_n in a polynomial-time computation? Maybe the oracle can provide answers to the necessary parts of α_n during the computation. For instance, if M accesses the advice string α_n in certain positions during its computation, the oracle can contain those positions. So, for each (n, i), where i is a bit position in α_n, include (n, i, b) in S_L if the ith bit of α_n is b. But similar to previous thoughts.\n",
      "\n",
      "But then S_L could be S parse by using the following encoding: for each n, and each i from 1 to p(n), there is an entry (n, i) concatenated with the bit b. The length of each string is O(log n + log i). Since p(n) is polynomial in n, log i is O(log n). Thus, each string in S_L is of length O(log n). The number of such strings per n is p(n), which is polynomial in n, so for each length m=O(log n), there are p(n) = poly(2^{m}) = exponential in m. Therefore, the set S_L contains exponentially many strings for length m, hence not sparse.\n",
      "\n",
      "Alternative Approach: Use hashing or a data structure. Suppose for each n, combine the whole α_n into a single string, and find a collision-resistant hash of α_n. Not sure.\n",
      "\n",
      "Wait, think about P_angel: the original TM M is using α_n to decide L, so for an input x, M(x, α_n) runs in poly(|x|) time. Let's simulate M on input x with the advice α_n. The advice is a fixed string. If we can rewrite the computation of M(x, α_n) as an oracle machine that makes queries to an oracle S_L, which contains α_n's in some sparse form.\n",
      "\n",
      "But since M runs in poly-time, it can only access poly many bits of α_n. Suppose the original machine M accesses at most q(|x|) bits of α_n during its computation. Then, S_L can store all possible bits that M would ever access for any x of length n. Wait, but x varies and the bits accessed depend on x.\n",
      "\n",
      "Alternatively, perhaps encode the entire advice α_n into a single oracle query. When M runs, the use of the advice string's bits would require q(n) queries, but if the advice is encoded in such a way that a single query can retrieve the entire α_n.\n",
      "\n",
      "Here's an idea: Since the TM M, when processing x ∈ {0,1}^n, uses the advice string α_n. If α_n can be passed as part of the oracle through a unique identifier, perhaps the query is a pair (n, i) where i is a bit position in α_n, but then S_L contains these pairs (n,i) with their values. Wait, that was the prior idea which leads to non-sparsity.\n",
      "\n",
      "Alternatively, define S_L as follows: For each α_n, compute a tableau of the computation of M(x, α_n) for all x of length n. But then S_L would contain all accepting paths, which might not be helpful.\n",
      "\n",
      "Alternatively, recall that P_angel is equivalent to P/poly, but with the advice strings being generated non-uniformly. The class P/poly is equivalent to languages decidable by a polynomial-time Turing machine with a polynomial-length advice string, which can be arbitrary. It is known that P/poly contains undecidable languages (as the advice can be uncomputable), but when the advice is computable in poly-time (as in P_bad-angel), it's equal to P.\n",
      "\n",
      "In our case, the problem wants to show that L ∈ P_angel implies that L is decidable by a poly-time machine with a sparse oracle S_L. It is a standard result in complexity theory that P/poly is exactly the class of languages decidable by a polynomial-time machine with a sparse oracle. (This is the Meyer, I think, theorem.) So perhaps this is the intended approach. That if a language is in P/poly, then it is decidable with a sparse oracle. Since P_angel is exactly P/poly (canonical example given was unary languages including undecidable ones), then L ∈ P_angel is L ∈ P/poly, and hence there exists a sparse oracle S_L so that L is in P^{S_L}.\n",
      "\n",
      "Yes, the standard result is that a language is in P/poly if and only if it is Turing-reducible in polynomial time to a sparse set. So that's probably the answer here.\n",
      "\n",
      "Hence, you just reference this result. However, since the problem asks to show it, not just cite, perhaps construction:\n",
      "\n",
      "Given L ∈ P_angel, then L has a poly-size advice {α_n} and a poly-time M that uses α_n to decide L. The standard construction is to build a sparse oracle S_L consisting of all tuples <n, i, b>, where the i-th bit of α_n is b. But in order to keep S_L sparse, we need to ensure that for each length m, there are poly(m) strings.\n",
      "\n",
      "Wait, if we encode <n, i, b> as a binary string where n and i are represented in binary, then for each n and i < p(n), the length of the string would be O(log n + log p(n)) = O(log n) (since p(n) is poly(n), so log p(n) = O(log n)). Therefore, each string in S_L for parameters n and i would have length O(log n).\n",
      "\n",
      "The number of strings per n is p(n) (since α_n has length p(n)), leading to p(n) strings for each n. For length m = O(log n), how many strings per m? For each m ≈ log n, n ≈ 2^m, so p(n) = p(2^m) which is 2^{O(m)} (since p is a polynomial), which is exponential in m. Hence, the total number of strings of length m is 2^{O(m)}, which is not polynomial.\n",
      "\n",
      "Therefore, this encoding causes S_L to be not sparse. Alternatively, use a more intelligent encoding.\n",
      "\n",
      "Alternatively, encode α_n into a special string for n. For example, for each n, let s_n be the binary encoding of α_n. But α_n has length p(n), so s_n is a string of length p(n). Thus, S_L = { s_1, s_2, s_3, ... }, with each s_n of length p(n). Since for each m, the number of n such that p(n) = m is at most one (if p is injective), then for each length m, S_L contains at most one string. Hence, S_L is sparse.\n",
      "\n",
      "In this case, the oracle TM can, given input x of length n, retrieve s_n from the oracle (but how?), by generating all strings of length p(n) and querying them until it finds which one is in S_L. However, this requires exponential time.\n",
      "\n",
      "Alternatively, assume the oracle can provide s_n in one query. That is, if the oracle encodes each s_n as the string for length p(n), then to determine α_n, the machine can try to generate all possible s of length p(n) and check whether s ∈ S_L. But this is impossible in polynomial time.\n",
      "\n",
      "So perhaps the answer is not straightforward, hence why reference to the P/poly theorem. The connection between sparse oracles and P/poly is established that a language is in P/poly if and only if it is polynomial-time Turing reducible to a sparse set. So that's the main theorem here.\n",
      "\n",
      "So, the required S_L is such a sparse set that allows L to be reduced. So probably the intended answer is to cite/to use this theorem.\n",
      "\n",
      "Hence, the proof outline is:\n",
      "\n",
      "Since L is in P_angel, which is equivalent to P/poly, by the mentioned theorem, it can be decided by a polynomial-time TM with access to a sparse oracle. Therefore, S_L exists as required.\n",
      "\n",
      "Obviously, in an exam setting, need to explain more.\n",
      "\n",
      "Another angle: Let S_L contain the tuple <n, α_n >. But encode this in a way that for each n, there's a string of polynomial length. For instance, padding α_n with 1^log n. But still, unsure.\n",
      "\n",
      "Alternatively, this is similar to problem 1. If you consider countably infinite advice strings for all n, and represent them as a one-hot encoding, where each α_n is a unique string. As L is in P_angel, every input length uses some advice string α_n.\n",
      "\n",
      "\n",
      "I think I'm going in circles here. Given the time constraints, maybe for problem 3, since it's known in complexity theory that P/poly is equivalent to having a polynomial-time machine with a sparse oracle, and P_angel is basically P/poly, so the answer is invoking this theorem. Thus, the existence of such a sparse set S_L follows from the definition of P/poly.\n",
      "\n",
      "Therefore, for the first problem, combining multiple sparse sets into one via tagging. For problem 2, P_bad-angel equals P, and NP doesn't unless NP=P. Problem 3 is about reducing P_angel (which is P/poly) to polynomial-time with a sparse oracle, which is standard.\n",
      "\n",
      "Number of samples with '**Final Answer**' in deepseek_thinking_trajectory: 6\n"
     ]
    }
   ],
   "source": [
    "# Count how many deepseek_thinking_trajectory fields contain \"**Final Answer**\" (case-insensitive)\n",
    "final_answer_count = 0\n",
    "for sample in dataset:\n",
    "    reasoning = sample.get(\"deepseek_thinking_trajectory\", \"\")\n",
    "    if r\"\\boxed\" in reasoning.lower():\n",
    "        final_answer_count += 1\n",
    "    else:\n",
    "        print(reasoning)\n",
    "        break\n",
    "print(f\"Number of samples with '**Final Answer**' in deepseek_thinking_trajectory: {final_answer_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ee327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gstmchen/miniconda3/envs/d1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 128, 'block_length': 32, 'temperature': 0.0, 'cfg_scale': 0.0, 'remasking': 'low_confidence'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:08<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use longllada to process s1K dataset and plot the attention map.\n",
    "\n",
    "# Assumptions:\n",
    "# - The LongLLaDA/llada/llada_wrapper.py is in the Python path or can be imported.\n",
    "# - The model weights and tokenizer are available at the specified path.\n",
    "# - The s1K dataset is already loaded as `dataset`.\n",
    "# - The SYSTEM_PROMPT variable is defined.\n",
    "# - The code is running in a notebook with matplotlib available.\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import the LLaDACausalLM wrapper\n",
    "from LongLLaDA.llada.llada_wrapper import LLaDACausalLM\n",
    "\n",
    "# Set up the model path and any required configs\n",
    "llada_model_path = \"GSAI-ML/LLaDA-8B-Instruct\"  # or your checkpoint path\n",
    "llada_kwargs = dict(\n",
    "    model_kwargs=dict(torch_dtype=\"auto\"),\n",
    "    max_seq_len=30000,\n",
    "    model_type=None,  # or 'llama', 'dream', etc. as appropriate\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "llada = LLaDACausalLM(path=llada_model_path, **llada_kwargs)\n",
    "\n",
    "# Load the s1.1K dataset\n",
    "dataset = load_dataset(\"simplescaling/s1K-1.1\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a5fce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:126081 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SYSTEM_PROMPT as in sft_trainer.py\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "Your reasoning here\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Select a sample from the dataset to process\n",
    "sample_idx = 0\n",
    "question = SYSTEM_PROMPT + \"\\n\\n\" + dataset[sample_idx][\"question\"]\n",
    "prompt = [{\"role\": \"user\", \"content\": question}]\n",
    "full_input = llada.tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = [full_input]\n",
    "input_ids = llada.tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=llada.max_seq_len).input_ids.to(llada.model.device)\n",
    "\n",
    "# LLaDA does not support output_attentions=True and will raise an error if you try.\n",
    "# Instead, we can only generate output, and cannot extract attention maps directly.\n",
    "# We'll generate the model's output and print it.\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = llada.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=False,\n",
    "        output_scores=False,\n",
    "        past_key_values=None,\n",
    "        use_cache=False,\n",
    "    )\n",
    "\n",
    "# Decode and print the generated output\n",
    "generated_ids = output[0][input_ids.shape[1]:]  # skip prompt tokens\n",
    "generated_text = llada.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(\"Generated output:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769f70d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[126080, 126346,   3840, 126347,    198,    198,  71808,    296,    268,\n",
       "           2538,   6149,     25,    198,     27,  23847,    283,     29,    198,\n",
       "           6943,  25306,   1655,    198,   1263,  23847,    283,     29,    198,\n",
       "             27,  31113,     29,    198,   1152,    198,   1263,  31113,     29,\n",
       "            198,    198,    198,  19781,    259,  17489,   1788,     11,   4223,\n",
       "            403,    409,    259,  15004,    296,  15506,   3935,    301,  13125,\n",
       "            268,   1963,    300,    268,  10559,  72974,    301,  24080,     13,\n",
       "           1741,   1099,   1494,  17489,   5803,   1745,    220,     15,    301,\n",
       "            220,     16,    627,    558,     17,     15,   1906,   7670,     92,\n",
       "          99968,    367,    268,  10559,   1963,     30, 126348, 126346,    598,\n",
       "          10450, 126347,    198,    198,    198,    198, 126081]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy on s1K dataset\n",
    "from test_s1_accuracy import load_model_and_tokenizer, evaluate_dataset\n",
    "\n",
    "# Load model and tokenizer (using default Qwen2.5-7B-Instruct)\n",
    "model, tokenizer = load_model_and_tokenizer(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "# Test on a subset of the dataset (first 10 samples for quick testing)\n",
    "print(\"Testing accuracy on s1K dataset...\")\n",
    "results, accuracy, correct_count, total_samples = evaluate_dataset(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    dataset=dataset, \n",
    "    max_samples=10\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
